# Data Engineering Nano Degree- Project 1

## 1. Background & Objective of the Project

Data is an important part of making business decisions. However, it requires an organized approach that separates the data capturing process from generating business analytics to ensure data integrity and efficiency. To perform business analytics, sometimes it is important to combine data from multiple sources that makes business sense. Moving data between systems often requires multiple steps and that may include: copying data, moving it to cloud, change the format and join with other data sources. A data pipeline can be described as the combination of all these steps, and its job is to ensure that these steps all happen reliably to all data [Ref-1]. 

A startup called **Sparkify** wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app. They'd like a data engineer to create a Postgres database with tables designed to optimize queries on song play analysis and bring you on the project. 

The objective of this project is to create a database schema and an ETL pipeline for the desired analysis that Sparkfy wants to perform. Throughout the project, some of the fundamental ideas of data modelling with Postgres is performed to build an ETL pipeline using Python. As a part of the project fact and dimension tables for a star schema has been defined, and an ETL pipeline has been created to transfers data from files in two local directories into these tables in Postgres using Python and SQL.


## 2. Installations

This project was written in Python, using Jupyter Notebook on Anaconda. The relevant Python packages for this project are as follows:

* os
* glob
* psycopg2
* pandas

To follow the steps it is recommended to install Anaconda, a pre-packaged Python distribution that contains all of the necessary libraries and software for this project.

Psycopg is a PostgreSQL adapter for the Python programming language. It is a wrapper for the libpq, the official PostgreSQL client library. To install psycopg2 you need to run the following in the command prompt : **`pip install psycopg2`**


## 3. Dataset

A. **Song Dataset**: The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

B. **Log Dataset**: The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from a music streaming app based on specified configurations.The log files used in the dataset are partitioned by year and month. 



## 4. Database schema design and ETL pipeline 

Entity Relationship Modelling (ER Modeling) and Dimensional Modeling (DM Modeling) are two basic data modeling techniques. The ER modelling consists of the basic concepts like entities, attributes and relationships and it is effective to keep data in normalized form and proves to be very effective when it comes to transaction based system. 

However, as we make  analytical decision for business it is required to JOIN tables in ER which proves to be slow as it involves JOINING tables that contains a large amount of data and this is a case that calls for dimensional modelling. 


**Dimensional modeling** is primarily used to support online analytical processing also known as OLAP. Dimensional modelling also involves three basic concepts- facts, dimensions and measures. The star model is the basic and most widely used structure for a dimensional model. The star schema has one large central table also known as Fact table, and a set of smaller tables called the dimension tables.    

**A Fact** can be described as- "a collection of related data items, which consist of measures and context data. Each fact typically represents a business item, a business transaction, or an event that can be used in analyzing the business or business processes". In this case a business transaction is a song that is played by a particular user.


Hence, the **Fact Table** contains records in log data associated with song plays such as **songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location and user_agent**.

**Dimension** on the other hand -" are the parameters over which we want to perform Online Analytical Processing (OLAP)" **[Ref-6]**. 

In the context of our startup- Sparkify who wants to do a song play analysis to optimize queries on song play, we have defined the following Fact and Dimension tables :

**Dimension Table**: The dimension table contains the attributes related to the song and help to store the context of the business. The dimension table contains the following items-


+ **users**: contain the users in the app and have the following columns- user_id, first_name, last_name, gender, level

+ **songs**: contains the songs in music database and have the song_id, title, artist_id, year, duration

+ **artists** - contains the artists in music database. It has the properties of artist_id, name, location, lattitude, longitude

+ **time** - contains the timestamps of records in songplays broken down into specific units
start_time, hour, day, week, month, year, weekday

Here is how the Entity Relationship Diagram looks like:

![](Database_ER_Diagram.png)



## 5. File Descriptions

1. `create_tables.py` drops and creates the tables for the new created database. This file resets the tables before each time the ETL scripts are run. 

2. `test.ipynb` displays the first few rows of each table to check the created database.

3. `etl.ipynb` reads and processes a single file from song_data and log_data and loads the data into the tables. This notebook is also a step by step demonstration of how the ETL process for each of the tables was created.

4. `etl.py` reads and processes files from song_data and log_data and loads them into the tables. This is filled out based on the work in the ETL notebook.

5. `sql_queries.py` contains all the sql queries, and is imported into the files mentioned in 1, 2, and 3 above.


## 6. Steps

1. With CREATE statements in `sql_queries.py` appropraite tables have been created.

2. To make sure there is no existing tables with a similar titile DROP statements have been used in `sql_queries.py`.

3. The file `create_tables.py` is run to create the database and tables.

4. To confirm the creation of all the tables with the correct columns `test.ipynb` file is run. It is **important** to to click **"Restart kernel"** to close the connection to the database after running this notebook.

5. `etl.ipynb` notebook details out the development of ETL processes for each table. At the end of each table section running the `test.ipynb` confirms that records are successfully inserted into each table.  At the end of the notebook, running the `test.ipynb` confirms that all the records are successfully inserted into each table. It is crucial to remember to **rerun** `create_tables.py` to reset the tables before each time this notebook is run. 


6. Once the `etl.ipynb` is completed, the code has been transferred to `etl.py`, to process the entire datasets. It is required to run `create_tables.py` before running `etl.py` to reset your tables. 

7. Running the `test.ipynb` confirms all the records were successfully inserted into each table and the creation of the ETL pipeline that transfers data from files in two local directories into facts and dimension tables in Postgres using Python and SQL.


## 7. Run

The `create_tables.py` files and  `etl.py` need to be run using the following commands in the terminal : 

**`python create_tables.py`**

**`python etl.py`**


## 8. References

1. [What is a data pipeline](https://www.dremio.com/what-is-a-data-pipeline/)

2. Course Content : [Udacity Data Engineerg Nanodegree](https://eu.udacity.com/course/data-engineer-nanodegree--nd027)

3. Basic Syntax for using [Markdown](https://www.markdownguide.org/basic-syntax/) 

4. For selecting the appropriate datatype: Tuotial [PostgreSQL](http://www.postgresqltutorial.com/)

5. For general issues related to python and dataframe: [Stackoverflow](https://stackoverflow.com/)

6. Style Guide for Python Code [PEP 8](https://www.python.org/dev/peps/pep-0008/) has been followed

7. Basics explanation of [Fact and Dimension Table](https://medium.com/@BluePi_In/deep-diving-in-the-world-of-data-warehousing-78c0d52f49a)